{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/macgoral/hcai_exercises/blob/main/QLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fbmg-N6kVEK",
        "outputId": "b2a0fe39-cf1d-48f8-9bbe-1b7f5a24d7d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install gymnasium pygame numpy --quiet\n",
        "\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "# we create the environment\n",
        "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
        "#env = gym.make(\"Taxi-v3\")\n",
        "\n",
        "n_observations = env.observation_space.n\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "# We start with a Q table of \"all zeros\"\n",
        "Q_table = np.zeros((n_observations, n_actions))\n",
        "print(Q_table)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7n4xnE4H6AL",
        "outputId": "2ba0962e-25bf-4e8a-ea73-556ee8bc9417"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n",
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n",
            "Mean rewards\n",
            "1000 : mean episode reward:  0.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# the number of episodes we use for training\n",
        "n_episodes = 1000\n",
        "\n",
        "# maximum of transitions per episode\n",
        "max_traj_per_episode = 1000\n",
        "\n",
        "# Epsilon-Greedy with start value 1\n",
        "epsilon = 1\n",
        "\n",
        "# value for decreasing Epsilon per step\n",
        "epsilon_decay = 0.05\n",
        "\n",
        "# minimum of exploration proba\n",
        "min_epsilon = 0.000000001\n",
        "\n",
        "# discounted factor\n",
        "gamma = 0.99\n",
        "\n",
        "# learning rate\n",
        "lr = 0.01\n",
        "\n",
        "total_rewards_episode = list()\n",
        "rewards_per_episode = list()\n",
        "\n",
        "\n",
        "# we train for a number of episodes\n",
        "for e in range(n_episodes):\n",
        "    # every time we start, we reset the environment\n",
        "    # (Gymnasium now returns (observation, info))\n",
        "    current_state, info = env.reset()\n",
        "    done = False\n",
        "\n",
        "    # we sum the individual rewards an agent gets per episode\n",
        "    total_episode_reward = 0\n",
        "\n",
        "    for i in range(max_traj_per_episode):\n",
        "        # Epsilon-Greedy calculation - do we explore, or exploit?\n",
        "        if np.random.uniform(0, 1) < epsilon:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            action = np.argmax(Q_table[current_state, :])\n",
        "\n",
        "        # we perform the selected action on the environment and get\n",
        "        # (1) the next state we are landing in,\n",
        "        # (2) the reward of the last action,\n",
        "        # (3) information if the episode ended\n",
        "        # Gymnasium returns: observation, reward, terminated, truncated, info\n",
        "        next_state, reward, terminated, truncated, info = env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        # After each step, we update our Q-value for the last transition according to Q-learning\n",
        "        Q_table[current_state, action] = Q_table[current_state, action] + lr * (\n",
        "            reward + gamma * max(Q_table[next_state, :]) - Q_table[current_state, action]\n",
        "        )\n",
        "\n",
        "        # SARSA\n",
        "        # next_action = np.argmax(Q_table[next_state, :]) if np.random.uniform(0,1) > epsilon else env.action_space.sample()\n",
        "        # Q_table[current_state, action] = Q_table[current_state, action] + lr * (\n",
        "        #     reward + gamma * Q_table[next_state, next_action] - Q_table[current_state, action]\n",
        "        # )\n",
        "\n",
        "        # After each step, we add the reward of the last action (if any) to our episode reward\n",
        "        total_episode_reward = total_episode_reward + reward\n",
        "\n",
        "        # If the last action yielded to the end of an episode, we stop and continue with a new run\n",
        "        if done:\n",
        "            break\n",
        "        current_state = next_state\n",
        "\n",
        "    # We reduce epsilon to slowly move from exploration to exploitation\n",
        "    epsilon = max(min_epsilon, np.exp(-epsilon_decay * e))\n",
        "\n",
        "    # At the end of every episode, we store the cumulative reward that we got\n",
        "    rewards_per_episode.append(total_episode_reward)\n",
        "\n",
        "    if e % 1000 == 0:\n",
        "        print(epsilon)\n",
        "\n",
        "\n",
        "\n",
        "# at the end of training, we also show how our Q-values have been updated\n",
        "print(Q_table)\n",
        "\n",
        "# We also show in steps of 1000 if and how the average reward has been increased\n",
        "print(\"Mean rewards\")\n",
        "for i in range(n_episodes // 1000):\n",
        "    print((i + 1) * 1000, \": mean episode reward: \",\n",
        "          np.mean(rewards_per_episode[1000 * i:1000 * (i + 1)]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bO-vVlURq-ea"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}